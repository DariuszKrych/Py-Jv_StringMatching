# Overall Team

## Findings
.md File Format: Got acquainted with the format conventions of a .md file while initially setting up the readme.md file initially and then updating it throughout the course of the project with headings, subheadings, lists and inline links on the project GitHub page.

Team communication: Learned through experience in the programming project throughout the project with work division and progress updates between Nathan Micallef and Dariusz Krych. Leading to co-ordination of all implemented string matching algorithms types including Naive, Native, Rabin-Karp, KMP & Boyer Moore Horspool being implemented in both the Python and Java side for comparison.

The main concept is string matching for a pattern from a given text string. The text strings are being taken from the steam 2017 review database. The main database is split into separate game review csv files which are up to a few dozen thousand rows as compared to the databases total ~6.4 million rows, these are loaded into the algorithm through a function which retrieves the data.

## Important Remarks
when testing the algprithms using the exact same dataset (Valkyria_Chronicles_reviews.csv) and using the exact same word (Edelweiss) the difference in time between the 2 laguages start to show. 
Naive:    Python- 5.036 seconds     Java- 0.143 seconds
Native:   Python- 3.449 secodns     Java- 0.052 seconds(please note that in Python there was a second native algorithm that did it in approx. 4 seconds)
RabinKarp:Python- 6.714 seconds     Java- 0.126 seconds (please note python had second algorithm that did itin 19 seconds)
KMP:      Python-  2.438 seconds    Java-0.123 seconds     
Boyer Moore Horsepool: Python- 1.522 seconds Java:0.035 seconds    
as we can see in these findings, the java algorithms are much faster than python. this may be because python is an interpreted language, meaning it runs through the code on execution, while java compiles its code making it faster for larger amounts of data.
&zwnj;
# Dariusz Krych
## Findings
A reference dictionary/table is a helpful tool for improving the speed of parsing through large amount of data or skipping calculations and referencing their results directly. Although the code which operates the reference dictionary/table has to be thought out as well, otherwise the benefits of the reference dictionary/table can easily be nullified. For example the Python code I wrote needs to filter certain rows of data out and save them, I implemented a reference dictionary which allows for skipping through up to x millions rows of data and only parsing through the rows which are to be saved instead of parsing through all of them via brute force with filtering. Although if I use csv reading built in to Python even with the reference dictionary, saving the desired rows can be slower than using no reference dictionary and pandas csv reading if the number of rows which needs to be skipped is too great. The final solution uses pandas csv reading and a reference dictionary combining both fastest options.

The Boyer Moore Horspool string matching algorithm is the fastest algorithm most of the time from my own testing with real world data (real steam review strings from 2017). The next contender which comes to mind would be the full Boyer Moore string matching algorithm which although can be theoretically faster (Better theoretical best time complexity) it tends to be slower with real world data. The Horspool version only uses the bad character rule which is very fast to compute and allows for some skipping of comparisons. The full version of the algorithm uses the bad character and good syntax rule with the good syntax rule. During the time it takes to figure out whether the good syntax or bad character (tables of both are based off of the pattern) has a larger possible character skip more time has passed than simply applying the bad character rule only. I have tested this myself with the full string matching algorithm although when I noted how complex the good syntax rule is for making the algorithm only theoretically faster I decided to not include it in the final version which is why I am noting the differences here instead of in docstrings where my thought regarding the other string matching algorithms are written.

Having at least one function in between the database and the functions which will make use of the databases data in a meaningful manner in this case the string matching algorithms is a must. If the the algorithms were to call the database directly programming any of it would be painful. By having at least one function in between the algorithms and database, only the function(s) in between need to be changed allowing for simpler code maintenance. This circumvents common coupling where several functions act upon common data and allows for more desirable data coupling for the string matching algorithms where only necessary data is exchanged with a function in between getting the data from the csv files (In this case split off from the database csv files) and then sending it over to the string matching algorithms.

Data always comes first. The quality of the data will determine how useful the program is no matter how great the algorithms or data processing is. The first thing I did when the project started was to look at what string matching algorithms are to determine what kind of data would be useful and immediately acquire data which would suit the needs of the project. In this case real world data of all steam reviews written in specifically 2017. The age of the data for the purposes of this project does not really matter, what does is that it is real world data which will give more value/weight to the results generated by the string matching algorithms when compared to using synthetically generated data (some randomly generated characters etc). Sidenote: This has a side-effect of making interacting with the project notably fun as you can for example take your favourite game and search for a specific term in its reviews on steam or look for how many times bug is mentioned in Skyrim reviews compared to other games (as expected it is much more frequent per review) etc.

The Python code has been fully documented with a combination of comments and docstrings. The comments were added while the Python code was being written over the course of over 100 days from the beginning of October 2024 to the middle of January 2025. Then docstrings were added in January by request of the challenger to make understanding the code easier. I could understand my code after not interacting with it for the better part of a month through the Christmas break although I do have to work on making it clearer for others, with docstrings being what I feel is a step in the right direction. Although ideally my code should be self explanatory not only to myself but to anyone reading it. To this end I did try my best to include descriptive names which themselves explain what a variable or function does without having to read its code.

I wrote my code with maintainability in mind. With every function having a set input and output which other functions rely on. With no parts of code within one function changing some variable directly affecting a variable inside another function. The idea was to pass only necessary data or no data if possible simply calling a function to run through as seen with some of the functions regarding the dataset. If the functions do not affect each other internally and their inputs and outputs have pre-defined expectations then what is changed internally is completely irrelevant making modifications to the code for improving performance etc possible without hassle. The only data which is shared across many functions is data which is changed manually such as two dictionaries including names and descriptions for the algorithms as well as 

## Important Remarks
- A reference dictionary/table is a great tool for improving performance when you use it with the correct supporting code in a valid scenario.
- Do not be afraid of making use of libraries in Python especially if performance is a critical aspect.
- The Boyer Moore Horspool string matching algorithm is what I will turn to if I ever need to write a string matching algorithm from scratch again as it is simple to implement, and while not theoretically the fastest with real world data it tends to be.
- Make sure to always add docstrings with more detailed explanations of how and why code works the way it does to make interacting with it easier for anyone familiarising themselves with the project.
- Ensure the code is well structured so modifying it is easier.
&zwnj;
# Nathan Micallef
## Findings
While writing my code for this Project, I learnt just how much more versitile a programming language can be, and how much more functionalities are included in the Java api. For example: during this project i learnt that Java has a file reader system in its API, which i used to read CSV files and be able to run different string mathcing algorithms on each CSV file. It was impressive to see how much faster Java was using an object oriented apporach than python. Being a compiled language, Java doesnt have to run the code line by line everytime it wants to run the code, therfore it makes it much faster for larger file sizes. While doing this project, I tried experimenting with external APIs, specifically apeche poi. But while trying to get it to work i realised it isnt efficient because even if the api is downloaded on my IDE of java, it is a complicated set up to do as someone who doesnt undertsand anytihing about programming or someone who doesnt use the specific IDE of Intellij. Due to these drawbacks i refrained from using external APIs and stuck to java's API, which turned out to  be more than enough for this project. During this project I imported many of Javas API, some of which are the Exceptions for error handeling. I learnt alot about how error handeling works, and how important it is as a user of  program to not having randomly crash due to an input error and having to restart the entire program. in the final project this instant only happens once for an unexplained reason that on my specific device, my file path changes how one of my algorithms work and on the longer file path it crashes half way through using the program. this is only a problem with 1 specific algorithm using the long file path and I am unsure of why it occurs. the file reader and buffer reader in the java API worked really well in allowing the CSV files to be read well and much more efficiently. the file reader class uses a basic buffer and a limited charachter set to read the files, but the buffer reader allows for faster reading and better understanding, making it like an upgrade to the file reader. The java programs limitations are in the extreme. the first algorithm to break was the RabinKarp algorithm, we tested the algorithm with a CSV fil that held over a million steam reviews and surprisingly most algorithms gave the same match, but the RabinKarp algorithm didnt give the same number of mtches as the others when presented with a dataset that large.  With the algorithms that don’t require the CSV reader, their limit is approximately 80 characters. This could probably be improved upon with more optimised code.

## Important Remarks
-	Java Is really fast compared to python
-	Adding external APIs to a program like this complicates things too much to be viable
-	Use of Buffer reader and file reader, as well as exceptions
-	Limitation parameter of program large

